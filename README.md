# Congestion Prediction in Chip Design with Walks and Paritioning

This project explores congestion prediction in chip design by utilizing graph-based learning and partitioning techniques. By incorporating dummy connections and domain-aware clustering, the approach enhances long-range message passing in hypergraphs, which aims to improve the accuracy of congestion forecasts. The repository includes notebooks and Python scripts for analyzing DE-HNN data, training models, and experimenting with different means of long range communication and partitioning methods. These methods aim to refine congestion-aware design decisions, optimizing chip layouts for better performance and efficiency. The original model can be found [here](https://github.com/TILOS-AI-Institute/DEHNN) along with the paper found [here](https://arxiv.org/pdf/2404.00477) (Luo et al. (2024))

## Setup

1. Download the data titled `superblue.zip` from [here](https://zenodo.org/records/14599896) and place
the unzipped file in the `de_hnn/data` folder.

3. Install Python (3.9) and installing dependencies by running `pip install -r requirements.txt`

4. Place all the files in the `scripts` folder in the root directory of the 
DEHNN repository. These scripts can be run to perform analysis, create 
visualizations, or modify the processed data for the graph representation.

5. Replace the `train_all_cross.py` file in the `de_hnn` folder and 
`pyg_dataset.py` file in the `de_hnn\data` folder with those of the same name 
in the `modified` folder.

6. Run the notebook, analysis and model training scripts with the DEHNN 
environment.

## File Structure

- `src`: contains all the python scripts and exploratory notebooks
   - `scripts`: contains scripts that can be run from root directory of DEHNN 
    repository that either manipulate data or perform analysis
      - `valid_pairs.py`: script for creating valid_pairs.csv
      - `random_walk_rep.py`: modifies `pyg_data.pkl` in the processed DEHNN 
        data to connect source-destination pairs from the results of random 
        walks. Saves it to `pyg_data_modified.pkl`
      - `analyze_pairs.py`: performs surface level analysis and visualizations
         on the `valid_pairs.csv` data
      - `analyze_pairs_detailed.py`: performs more in depth analysis and 
       visualizations on the `valid_pairs.csv` data
   - `notebooks`: contains all notebooks used for exploratory analysis
      - `data_exploration.ipynb`: contains exploratory analysis of DEHNN 
        datasets and model architecture
   - `modified`: contains files that have been modified from the original DEHNN 
    repository that needs to be replaced
      - `pyg_dataset.py`: replace original file of same name in DEHNN repo in 
      the `de_hnn/data` folder
      - `train_all_cross.py`: replace original file of same name in DEHNN repo 
      in the `de_hnn` folder. 
   - `benchmarking`: contains all models trained
      - `baseline_model`: contains baseline DEHNN model 
- `data`: contains all data generated by scripts and notebooks in src
   - `valid_pairs.csv`: table of source, destination pairs that are 
    topologically far but physically close, along with their features
   - `vn_features_init.csv`: table of initial VN features for all designs
